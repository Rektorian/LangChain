{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "796c3d19-9974-41c8-9408-9f29ae6420bb",
   "metadata": {},
   "source": [
    "# Simple RAG application using LangChain, IBM Generative AI Python SDK, LLama and FAISS vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0949ee73-296c-43d6-97c9-cecc2f223743",
   "metadata": {},
   "source": [
    "<b>Retrieval Augmented Generation (RAG)</b>\n",
    "\n",
    "General-purpose language models can be fine-tuned to achieve several common tasks such as sentiment analysis and named entity recognition. These tasks generally don't require additional background knowledge.\n",
    "\n",
    "For more complex and knowledge-intensive tasks, it's possible to build a language model-based system that accesses external knowledge sources to complete tasks. This enables more factual consistency, improves reliability of the generated responses, and helps to mitigate the problem of \"hallucination\".\n",
    "\n",
    "Meta AI researchers introduced a method called Retrieval Augmented Generation (RAG) to address such knowledge-intensive tasks. RAG combines an information retrieval component with a text generator model. RAG can be fine-tuned and its internal knowledge can be modified in an efficient manner and without needing retraining of the entire model.\n",
    "\n",
    "RAG takes an input and retrieves a set of relevant/supporting documents given a source (e.g., Wikipedia). The documents are concatenated as context with the original input prompt and fed to the text generator which produces the final output. This makes RAG adaptive for situations where facts could evolve over time. This is very useful as LLMs's parametric knowledge is static. RAG allows language models to bypass retraining, enabling access to the latest information for generating reliable outputs via retrieval-based generation.\n",
    "\n",
    "source: https://www.promptingguide.ai/techniques/rag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f225758-33ca-4683-8a93-03c346281eb4",
   "metadata": {},
   "source": [
    "# 1. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c35b142-0f27-4310-a898-e1b2bee4c3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # IBM Generative AI Python SDK:\n",
    "from genai import Client, Credentials\n",
    "from genai.extensions.langchain import LangChainInterface\n",
    "from genai.extensions.langchain import LangChainChatInterface\n",
    "from genai.schema import (\n",
    "    DecodingMethod,\n",
    "    ModerationHAP,\n",
    "    ModerationHAPInput,\n",
    "    ModerationHAPOutput,\n",
    "    ModerationParameters,\n",
    "    TextGenerationParameters,\n",
    ")\n",
    "\n",
    "# Langchain packages:\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import FAISS # Faiss is a library for efficient similarity search and clustering of dense vectors.\n",
    "\n",
    "# Supporting packages:\n",
    "from typing import Any, Dict, List\n",
    "import inspect\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35283c4-5b57-4c33-9fbd-ac23d179ea30",
   "metadata": {},
   "source": [
    "# 2. Login to IBM Cloud and select LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3a0dcf0-9439-4752-939a-b31080a865bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.environ.get('IBM_BAM_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "602cef8e-efd2-494c-8a06-894b02db29b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = Credentials(api_key=api_key, api_endpoint=\"https://bam-api.res.ibm.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70a4b3d2-8fc0-44a7-a2a6-8b7b3c585de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/llama-3-8b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73643ea8-ebed-4ae5-9e42-b2ce49752be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(credentials=credentials)\n",
    "llm = LangChainChatInterface(\n",
    "    client=client,\n",
    "    model_id=model_name,\n",
    "    parameters=TextGenerationParameters(\n",
    "        decoding_method=DecodingMethod.GREEDY, # Uses a more deterministic method to ensure accuracy and coherence.\n",
    "        temperature=0.3, # Lower temperature to make outputs more deterministic and less random\n",
    "        top_k=10, # Considers the top 10 tokens, balancing variety and accuracy.\n",
    "        top_p=0.9, # Ensures the tokens are within the 90% cumulative probability\n",
    "        max_new_tokens=800,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aabed41-eabb-4ea8-8746-6564e2cc35c1",
   "metadata": {},
   "source": [
    "# 3. Define Custom Embeddings class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9766a408-e958-479b-9cb6-da0f0eca521a",
   "metadata": {},
   "source": [
    "Now, in order to us to build our RAG application, we must define our embedding mechanism to store our dataset in vector database, that can be later queried for answers.\n",
    "\n",
    "For this purpose, we're going to define our <b>CustomEmbeddings</b> class which inherits from the Embeddings class provided by LangChain, and implements the required methods. We will base our custom class on <b>BAAI/bge-m3</b> sentence transformers library. \n",
    "\n",
    "Hereâ€™s how we can make CustomEmbeddings class to inherit from Embeddings:\n",
    "\n",
    "1. Import the necessary Embeddings class from LangChain.\n",
    "2. Ensure CustomEmbeddings inherits from Embeddings.\n",
    "3. Implement the required methods (embed_documents and embed_query) as per the Embeddings interface.\n",
    "\n",
    "First, let's inspect LangChain <b>'Embeddings' class</b>, so we know how to build our own 'class CustomEmbeddings':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "996eb678-b4e1-4a2e-8dcb-66d40556ef00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['class Embeddings(ABC):\\n',\n",
       "  '    \"\"\"Interface for embedding models.\"\"\"\\n',\n",
       "  '\\n',\n",
       "  '    @abstractmethod\\n',\n",
       "  '    def embed_documents(self, texts: List[str]) -> List[List[float]]:\\n',\n",
       "  '        \"\"\"Embed search docs.\"\"\"\\n',\n",
       "  '\\n',\n",
       "  '    @abstractmethod\\n',\n",
       "  '    def embed_query(self, text: str) -> List[float]:\\n',\n",
       "  '        \"\"\"Embed query text.\"\"\"\\n',\n",
       "  '\\n',\n",
       "  '    async def aembed_documents(self, texts: List[str]) -> List[List[float]]:\\n',\n",
       "  '        \"\"\"Asynchronous Embed search docs.\"\"\"\\n',\n",
       "  '        return await run_in_executor(None, self.embed_documents, texts)\\n',\n",
       "  '\\n',\n",
       "  '    async def aembed_query(self, text: str) -> List[float]:\\n',\n",
       "  '        \"\"\"Asynchronous Embed query text.\"\"\"\\n',\n",
       "  '        return await run_in_executor(None, self.embed_query, text)\\n'],\n",
       " 8)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsourcelines(Embeddings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd211bcf-a875-4b65-8c7e-330ff1e52c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "class CustomEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name='BAAI/bge-m3', use_fp16=True, output_dim=1024):\n",
    "        self.model = BGEM3FlagModel(model_name, use_fp16=use_fp16)\n",
    "        self.output_dim = output_dim\n",
    "    \n",
    "    def embed_query(self, text: str):\n",
    "        embeddings = self.model.encode([text], batch_size=1, max_length=1024)['dense_vecs']\n",
    "        if embeddings.shape[1] != self.output_dim:\n",
    "            raise ValueError(f\"Expected embedding dimension to be {self.output_dim}, but got {embeddings.shape[1]}\")\n",
    "        return embeddings[0].tolist()\n",
    "    \n",
    "    def embed_documents(self, texts):\n",
    "        embeddings = self.model.encode(texts, batch_size=len(texts), max_length=1024)['dense_vecs']\n",
    "        if embeddings.shape[1] != self.output_dim:\n",
    "            raise ValueError(f\"Expected embedding dimension to be {self.output_dim}, but got {embeddings.shape[1]}\")\n",
    "        return embeddings.tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8f58c4c-73dc-4ae8-8aae-f80e4a809a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3b9270ec13e4d1ca82fff452e6fc627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "custom_embeddings = CustomEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e14aa66-85ab-45ca-b3f1-6d0ca801041d",
   "metadata": {},
   "source": [
    "# 4. Create Vector Database and upload document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2f33e3-6679-4b7a-a902-aeafb376615a",
   "metadata": {},
   "source": [
    "Here is the document we want to upload into our vector db:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a95d6bc-da32-45d5-b0e4-25d0e16b3a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"data/Arduino Engineering Kit R2 FAQ updated.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e28d427-11c2-4ecd-bc7d-5ec27c166ec5",
   "metadata": {},
   "source": [
    "## Text splitting (chunking) for RAG applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7e8ef1-2557-49f8-a2e7-d0dfa9704049",
   "metadata": {},
   "source": [
    "We need to transform long text documents to smaller chunks that are embedded, indexed, stored then later used for information retrieval.\n",
    "\n",
    "Splitting documents into smaller segments called chunks is an essential step when embedding text into a vector store. RAG pipelines retrieve relevant chunks from the database, using similarity search metrics (like Euclidean distance, Cosine Similarity, Dot Product, etc.) and then serve the information to LLM, based on which it generates an answer for us.\n",
    "\n",
    "When we retrieve, we want to return the chunks that are semantically closest to the query. If chunks are not distinct enough semantically, then we may get information that was not asked for in the query, leading to lower quality results and higher LLM hallucination rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa116726-f4db-483c-bef0-3a39dfc9422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path=pdf_path) # load up the document, read it and chunk it.\n",
    "documents = loader.load() # variable that keeps our chunked pdf\n",
    "\n",
    "# We use character text_splitter to chunk our document\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=100, chunk_overlap=30, separator=\"\\n\"\n",
    ")\n",
    "docs = text_splitter.split_documents(documents=documents) # Do the required split on docs, now they are ready for embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c425a2-938f-4d2b-84b7-bb1bc64d9df2",
   "metadata": {},
   "source": [
    "## Upload data into FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b50f434-93f7-44da-8e1e-b7829628aed2",
   "metadata": {},
   "source": [
    "<b>FAISS Introduction:</b> \n",
    "\n",
    "FAISS is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning. Faiss is written in C++ with complete wrappers for Python/numpy. Some of the most useful algorithms are implemented on the GPU. It is developed primarily at Meta's Fundamental AI Research group.\n",
    "\n",
    "FAISS is also able to convert big vectors into objects that are small enough to be saved in the RAM.\n",
    "\n",
    "<b>In the below code</b> we provide two arguments:<br>\n",
    "(1) document and the embeddings, and<br> \n",
    "(2) this function will them into vectors.\n",
    "\n",
    "It's then going to take the vectors and store them on local machine in RAM as a vector store object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9fe8e475-46fb-48c0-aebd-db9e209d5330",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(docs, custom_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6051f7b0-1acc-4d0e-b40f-f0abcf3f9f11",
   "metadata": {},
   "source": [
    "We can also store it locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae486994-cb93-49da-a971-ee84d8966b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.save_local(\"faiss_index_db\") # our index name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aff39e-2464-4676-ac70-cf7735f2e01f",
   "metadata": {},
   "source": [
    "# 5. Query Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8164a5b-071d-4950-a02c-66d4a1ac12f5",
   "metadata": {},
   "source": [
    "Now we can query our db:\n",
    "\n",
    "1. <b>Loading the Vector Store with FAISS</b><br>\n",
    "new_vectorstore = FAISS.load_local(\"faiss_index_db\", custom_embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "    - <b>load_local(\"faiss_index_db\", custom_embeddings)</b>: Loads a FAISS index from a local file called \"faiss_index_db\". This index is used for efficient vector similarity search, custom_embeddings: Represents the embedding function or model used to generate embeddings that match those stored in the FAISS index.\n",
    "\n",
    "    - <b>allow_dangerous_deserialization=True:</b> A safety parameter that allows loading potentially untrusted serialized objects. Setting this to True should be done with caution as it may expose the system to risks if the serialized data is compromised.\n",
    "\n",
    "2. <b>Creating a Retrieval-based QA System</b><br>\n",
    "    - <b>RetrievalQA.from_chain_type(...)</b>: This method initializes a retrieval-augmented QA system.\n",
    "    - <b>llm=llm</b>: Specifies the language model (LLM) to be used for generating responses. llm should be an instance of a large language model like LLaMA.\n",
    "    - <b>chain_type=\"stuff\"</b>: Specifies the type of chain to use. In this context, \"stuff\" refers to a specific chain type where the relevant information is \"stuffed\" into the prompt for the language model to process.\n",
    "    - <b>retriever=new_vectorstore.as_retriever()</b>: Converts the new_vectorstore (FAISS index) into a retriever object that the QA system can use to find relevant documents based on the query. The retriever uses the FAISS index to find the most similar vectors (i.e., the most relevant documents).\n",
    "\n",
    "3. <b>Running the QA System</b><br>\n",
    "    - <b>qa.run(\"Which languages does the online platform support?\")</b>: Executes the QA system with the given query.\n",
    "        - The system uses the retriever to find relevant documents in the vector store.\n",
    "        - These documents are then passed to the LLM, which generates a response based on the retrieved information.\n",
    "    - <b>res</b>: Stores the output or answer generated by the QA system in response to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c139f967-3510-404c-a169-64240f4fcf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\P16899820\\anaconda3\\envs\\synthetic_data_generate_env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "new_vectorstore = FAISS.load_local(\"faiss_index_db\", custom_embeddings, allow_dangerous_deserialization=True) # load our vectorstore db from the local file\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=new_vectorstore.as_retriever())\n",
    "res = qa.run(\"Which languages does the online platform support?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c9b886f0-035b-428f-a153-865fe7cd6deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The online platform is currently available in English and Spanish.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
